{
  "version": "1.0.0",
  "createdAt": "2024-12-16T20:00:00Z",
  "environment": {
    "nodeVersion": "18.x",
    "platform": "Linux",
    "instanceType": "t3.large (recommended)",
    "ollama": "llama3.2 (CPU-based)"
  },
  "targets": {
    "api": {
      "health_check": {
        "target_p50_ms": 50,
        "target_p95_ms": 100,
        "target_p99_ms": 150,
        "description": "GET /health endpoint response time"
      },
      "list_endpoints": {
        "target_p50_ms": 200,
        "target_p95_ms": 500,
        "target_p99_ms": 1000,
        "description": "List service areas, keywords, jobs, pages (paginated)"
      },
      "create_endpoints": {
        "target_p50_ms": 300,
        "target_p95_ms": 750,
        "target_p99_ms": 1500,
        "description": "Create service areas, keywords, generation jobs"
      },
      "database_queries": {
        "simple_select_ms": 5,
        "complex_select_ms": 25,
        "insert_ms": 10,
        "update_ms": 10,
        "delete_ms": 5,
        "description": "SQLite query performance targets"
      }
    },
    "worker": {
      "content_generation": {
        "target_p50_ms": 15000,
        "target_p95_ms": 20000,
        "target_p99_ms": 30000,
        "description": "Time to generate single page (15-30 seconds on Ollama)",
        "note": "Includes: LLM inference + database write + file storage"
      },
      "job_processing": {
        "queue_to_start_ms": 100,
        "processing_to_complete_ms": 30000,
        "description": "Time from job creation to completion"
      },
      "parallel_capacity": {
        "target_jobs_per_minute": 4,
        "target_pages_per_hour": 240,
        "description": "Job throughput on t3.large with llama3.2"
      },
      "memory_usage": {
        "ollama_base_mb": 2048,
        "model_cache_mb": 2048,
        "node_process_mb": 512,
        "description": "Expected memory footprint (4GB model + overhead)"
      }
    },
    "dashboard": {
      "initial_load": {
        "target_p50_ms": 1000,
        "target_p95_ms": 2000,
        "target_p99_ms": 3000,
        "description": "Time for dashboard to load and display first data"
      },
      "page_transition": {
        "target_p50_ms": 300,
        "target_p95_ms": 500,
        "target_p99_ms": 1000,
        "description": "Client-side navigation between dashboard pages"
      },
      "data_refresh": {
        "target_p50_ms": 400,
        "target_p95_ms": 800,
        "target_p99_ms": 1500,
        "description": "Time to fetch and display updated data"
      }
    }
  },
  "slo": {
    "api_availability": 99.5,
    "worker_availability": 95.0,
    "p95_response_time_api_ms": 500,
    "p95_content_generation_time_ms": 20000,
    "description": "Service Level Objectives for v1.0"
  },
  "capacity": {
    "monthly_pages": 1300,
    "service_areas": 26,
    "keywords_per_area": 50,
    "concurrent_jobs": 2,
    "description": "Typical monthly capacity on single t3.large instance"
  },
  "cost_per_metric": {
    "cost_per_page_usd": 0.017,
    "cost_per_service_area_usd": 0.86,
    "cost_per_keyword_usd": 0.43,
    "total_monthly_cost_usd": 22.5,
    "description": "Cost efficiency metrics (includes EC2 + storage)"
  },
  "limits": {
    "max_job_execution_time_ms": 120000,
    "max_page_size_bytes": 1048576,
    "max_api_payload_bytes": 10485760,
    "database_connection_pool_size": 5,
    "worker_queue_max_jobs": 1000,
    "description": "System limits for v1.0"
  },
  "monitoring": {
    "metrics_collected": [
      "api_response_time_p50",
      "api_response_time_p95",
      "api_response_time_p99",
      "api_error_rate",
      "worker_job_processing_time",
      "worker_error_rate",
      "worker_queue_depth",
      "database_query_time_p95",
      "database_connection_pool_utilization",
      "system_cpu_utilization",
      "system_memory_utilization",
      "system_disk_utilization",
      "network_bandwidth_in_out",
      "ollama_inference_time",
      "ollama_memory_usage",
      "ec2_instance_health",
      "hourly_costs_running"
    ],
    "alert_thresholds": {
      "api_p95_response_time_ms": 500,
      "api_error_rate_percent": 1.0,
      "worker_error_rate_percent": 5.0,
      "worker_queue_depth": 100,
      "cpu_utilization_percent": 80,
      "memory_utilization_percent": 85,
      "disk_utilization_percent": 90,
      "hourly_cost_usd": 3.0
    }
  },
  "baseline_test_data": {
    "service_areas_created": 26,
    "keywords_created": 50,
    "generation_jobs_queued": 100,
    "test_duration_seconds": 300,
    "expected_pages_generated": 4,
    "description": "Baseline test parameters for performance validation"
  },
  "validation": {
    "last_validated": "2024-12-16",
    "validated_by": "E2E Integration Tests",
    "validated_against": {
      "instance_type": "t3.large",
      "ollama_model": "llama3.2:latest",
      "node_version": "18.x",
      "database": "SQLite"
    },
    "notes": [
      "Content generation time (15-30s) is dominated by LLM inference time on llama3.2",
      "API response times assume warm database connections and cached query plans",
      "Worker throughput limited by LLM inference (single GPU/CPU process per instance)",
      "Memory usage will increase if larger models are used (llama2, mistral, etc.)",
      "Performance may vary Â±10% based on EC2 instance load and network conditions",
      "Ollama model caching reduces subsequent inference by ~20%"
    ]
  }
}
